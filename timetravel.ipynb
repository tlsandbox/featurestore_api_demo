{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables\n",
    "import os\n",
    "from featurestore import *\n",
    "import datetime\n",
    "PROJECT_NAME = os.getenv(\"PROJECT_NAME\", \"demo_timetravelapi\")\n",
    "MAIN_USER_TOKEN = os.getenv(\"MAIN_USER_TOKEN\")\n",
    "SPARK_DEPS_AZURE = os.getenv(\"SPARK_DEPS_JAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define credentials for ingesting user's data\n",
    "S3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "S3_REGION = os.getenv(\"S3_REGION\")\n",
    "credentials = S3Credentials(S3_ACCESS_KEY, S3_SECRET_KEY, S3_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyspark==3.4.1 h2o-featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-azure:3.3.1\") \\\n",
    "    .config(\"spark.jars\", SPARK_DEPS_AZURE) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login and create client\n",
    "client = Client(API, secure=True)\n",
    "client.auth.set_auth_token(MAIN_USER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Delete the project if already exists\n",
    "try:\n",
    "    client.projects.get(PROJECT_NAME).delete()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timetravel API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project\n",
    "project = client.projects.create(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source for ingesting\n",
    "source = CSVFile(\"s3a://feature-store-test-data/customer_churn_data_based_on_dates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract schema from datasource\n",
    "schema = client.extract_schema_from_source(source, credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register a new feature set with the above schema without time travel column\n",
    "first_fs = project.feature_sets.register(schema, \"fs_without_time_travel\", \"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ingest data\n",
    "ingest = first_fs.ingest(source, credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since feature set is registered without time travel column, the time scope if ingested data is computed based on current\n",
    "# ingestion. Start and end date time of the scope are same in this case.\n",
    "# We can still perform filtering during retrieve, but that filtering is based on time of ingests\n",
    "# rather than based on data stored in the time travel column\n",
    "\n",
    "# Retrieve with boundaries based on ingestion time. \n",
    "ingestion_time = ingest._meta.ingestion_timestamp.ToDatetime().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "ref = first_fs.retrieve(ingestion_time, ingestion_time)\n",
    "ref.as_spark_frame(spark).show()\n",
    "\n",
    "# Retrieve with boundaries available in the time travel column do not have any impact in this case and we retrieve empty\n",
    "# feature set. This is because without time travel column, we operate on time of ingestions.\n",
    "ref = first_fs.retrieve(\"2021-04-02 00:00:00\", \"2021-04-03 00:00:00\")\n",
    "# In this case we can see the data is empty as expected\n",
    "ref.as_spark_frame(spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature set with time travel column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register a new feature set with the above schema with time travel column\n",
    "second_fs = project.feature_sets.register(schema, \"fs_with_time_travel\", \"description\", time_travel_column=\"Date\", time_travel_column_format=\"yyyy-MM-dd HH:mm:ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data\n",
    "ingest = second_fs.ingest(source, credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since feature set is registered with time travel column we can perform retrieve\n",
    "# filtering based on boundaries provided in the time travel column\n",
    "ref = second_fs.retrieve(\"2021-04-02 00:00:00\", \"2021-04-03 00:00:00\")\n",
    "ref.as_spark_frame(spark).show()\n",
    "\n",
    "# Retrieve with boundaries based on ingestion time leads to no data as we operate on data from time travel column\n",
    "ingestion_time = ingest._meta.ingestion_timestamp.ToDatetime().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "ref = second_fs.retrieve(ingestion_time, ingestion_time)\n",
    "# The data are expected to be empty\n",
    "ref.as_spark_frame(spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.projects.get(PROJECT_NAME).delete()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
