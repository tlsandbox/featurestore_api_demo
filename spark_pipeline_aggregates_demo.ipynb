{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived feature set based on Spark pipeline with aggregated window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve environment variables\n",
    "import os\n",
    "PROJECT_NAME = os.getenv(\"PROJECT_NAME\", \"demo_spark_pipeline_with_aggregates\")\n",
    "REFRESH_TOKEN = os.getenv(\"REFRESH_TOKEN\")\n",
    "SPARK_DEPS_AZURE = os.getenv(\"SPARK_DEPS_JAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install featurestore and other dependencies\n",
    "! pip install h2o-featurestore pyspark==3.4.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up featurestore client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login and create a client\n",
    "from featurestore import *\n",
    "client = Client(API, secure=True)\n",
    "client.auth.set_auth_token(REFRESH_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-azure:3.3.1\") \\\n",
    "    .config(\"spark.jars\", SPARK_DEPS_AZURE) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define credentials for external data source\n",
    "S3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "S3_REGION = os.getenv(\"S3_REGION\")\n",
    "credentials =  S3Credentials(S3_ACCESS_KEY, S3_SECRET_KEY, S3_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete project if it already exists\n",
    "try:\n",
    "    client.projects.get(PROJECT_NAME).delete()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = client.projects.create(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract schema from the external data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = CSVFile(\"s3a://feature-store-test-data/allyears2k_headers_with_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract schema \n",
    "schema = client.extract_schema_from_source(source, credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regiser a feature set\n",
    "fs = project.feature_sets.register(schema, \"fs_spark_pipeline_agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data into a feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data\n",
    "fs.ingest(source, credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a spark pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark pipeline using airlines dataset, that shows the average delay in arrival flights for the last 30 days\n",
    "from featurestore import SparkPipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml import Pipeline\n",
    "from featurestore import SparkPipeline\n",
    "query = \"select window.start AS start_time, window.end AS end_time, avg(ArrDelay) AS ave_arr_delay from __THIS__ group by window(Date, '30 days')\"   \n",
    "transformer = SQLTransformer(statement=query)\n",
    "spark_pipeline = Pipeline(stages=[transformer])\n",
    "pipeline_transformation = SparkPipeline(spark_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract schema from the spark pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters to extract derived schema are: Input feature set and spark pipeline transformations\n",
    "spark_pipeline_schema = client.extract_derived_schema([fs], pipeline_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_pipeline_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a derived feature set using spark pipeline schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_spark_pipeline = project.feature_sets.register(spark_pipeline_schema, \"derived_fs_spark_pipeline\", time_travel_column = \"end_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for data from input feature set to be propagated into derived\n",
    "from featurestore.core.job_types import INGEST\n",
    "jobs = fs_spark_pipeline.get_active_jobs(INGEST)\n",
    "if len(jobs) > 0:\n",
    "   jobs[0].wait_for_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_spark_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve feature set as a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_fs = project.feature_sets.get(fs_spark_pipeline.feature_set_name)\n",
    "df = latest_fs.retrieve().as_spark_frame(spark)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.projects.get(PROJECT_NAME).delete()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
